{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHoWZmQNDh05I5i8vfgHEA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rosehunnie/github-slideshow/blob/master/Assignment1complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 1.01: Basic Text analytics**\n",
        "\n",
        "Basic text analytics on some given text data. Search, index, finding position of a given word."
      ],
      "metadata": {
        "id": "vcC2b7nnVW-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'The quick brown fox jumps over the lazy dog'\n",
        "sentence\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ohg13I-GS6Kp",
        "outputId": "194a7bf1-f6e8-499a-e7e9-e81226272de5"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The quick brown fox jumps over the lazy dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_word(word, sentence):\n",
        "    return word in sentence\n",
        "find_word('quick', sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO87iMy3SeXs",
        "outputId": "68fd096c-b2b2-4397-f8d7-1f1d6f67cd72"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_index(word, text):\n",
        "    return text.index(word)\n",
        "get_index('fox', sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nnv1PTPSSeF-",
        "outputId": "9cce05eb-66d1-4ceb-82a7-46942c63e795"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_index('lazy', sentence.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZ_kA6a_Sd8f",
        "outputId": "834dc1c0-9845-4e76-8d41-541f4d6c6a8f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word(text,rank):\n",
        "    return text.split()[rank]\n",
        "get_word(sentence,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "f58wqlKMSdw8",
        "outputId": "0f408a46-d20c-492b-bc9a-22236b22b60f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'brown'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_word(sentence,2)[::-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EsQ1VhZLSdkr",
        "outputId": "9e14776b-a8bc-40b0-8633-9095b06a60fc"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'nworb'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_words(text):\n",
        "    \"\"\"\n",
        "    This method will concat first and last\n",
        "    words of given text\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    first_word = words[0]\n",
        "    last_word = words[len(words)-1]\n",
        "    return first_word + last_word\n",
        "concat_words(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qBoPGePGUlYQ",
        "outputId": "ab97862b-e43d-4fe8-9d3a-c96c43cbc0a5"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Thedog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_even_position_words(text):\n",
        "    words = text.split()\n",
        "    return [words[i] for i in range(len(words)) if i%2 == 0]\n",
        "get_even_position_words(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqdsT5V-UqMe",
        "outputId": "afd718fe-5773-4f4d-e1a2-ab9d80550637"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'brown', 'jumps', 'the', 'dog']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_last_n_letters(text, n):\n",
        "    return text[-n:]\n",
        "get_last_n_letters(sentence,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oN3LkOQhUt_2",
        "outputId": "9351e607-713f-437e-a92f-d49725ae1392"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_reverse(text):\n",
        "    return text[::-1]\n",
        "get_reverse(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ybhG-SZ6Ux-9",
        "outputId": "228fb116-d132-4f4a-a95b-210a9e472d61"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'god yzal eht revo spmuj xof nworb kciuq ehT'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 1.02: Tokenization of a simple sentence**\n",
        "\n",
        "Tokenize the words in a given sentence using NLTK library."
      ],
      "metadata": {
        "id": "jhUtZznOWNdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    return words"
      ],
      "metadata": {
        "id": "6K7DFtJKWZyB"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_tokens(\"I am reading NLP Concepts.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv3Mm2BdWfaW",
        "outputId": "80a35ed5-c84c-4b0f-c587-ce23dfafedf1"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'reading', 'NLP', 'Concepts', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "['I', 'am', 'reading', 'NLP', 'Fundamentals', '.']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNAid50MWjB2",
        "outputId": "c8f0fddf-9628-43b2-c859-0deb35db35ea"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'am', 'reading', 'NLP', 'Fundamentals', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 1.03: PoS Tagging**\n",
        "\n",
        "Find the PoS for each word in the sentence."
      ],
      "metadata": {
        "id": "M0WTd2IAQAuE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mKX6qoIbQAau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, download\n",
        "download(['punkt','averaged_perceptron_tagger','stopwords'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KTh5-I7wXRE",
        "outputId": "b8b304d4-f787-409f-ec41-d28a6e3453c3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    return words"
      ],
      "metadata": {
        "id": "Nm04ImwlxRVZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_tokens(\"I am reading NLP Concepts.\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0C43lp7xrbN",
        "outputId": "91c3ba4b-9db6-4607-bb92-002315514ced"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'reading', 'NLP', 'Concepts', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, pos_tag"
      ],
      "metadata": {
        "id": "HbW11tTxxs59"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    return words"
      ],
      "metadata": {
        "id": "brEZawMJyTq0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words  = get_tokens(\"I am reading NLP Concepts\")\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGTkWNvryYac",
        "outputId": "2b7a479b-44da-4ed9-e281-d8d39f4057ba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'reading', 'NLP', 'Concepts']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pos(words):\n",
        "    return pos_tag(words)\n",
        "get_pos(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsS4tdgZyjEt",
        "outputId": "f7f53b2f-6830-42b1-8a74-540928b8da56"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('am', 'VBP'),\n",
              " ('reading', 'VBG'),\n",
              " ('NLP', 'NNP'),\n",
              " ('Concepts', 'NNP')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 1.04: Stop Word Removal**\n",
        "\n",
        "Check the list of stop words in the nltk library. then filter out the stop words"
      ],
      "metadata": {
        "id": "CCCzDX1KQVWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import download\n",
        "download('stopwords')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz2VzArIyoq2",
        "outputId": "38870cd1-48f9-43ec-800e-ab7ef14ab070"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "yCbSdEjnywKt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiDFYv7nzCwv",
        "outputId": "5e7dd4b7-dbef-4be6-ffdc-bbbb760d25df"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I am learning Python. It is one of the \"\\\n",
        "           \"most popular programming languages\"\n",
        "sentence_words = word_tokenize(sentence)"
      ],
      "metadata": {
        "id": "wyp2wMr_zLaF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSH9a8CtzijC",
        "outputId": "f5dfb8bb-933b-40c5-a859-3c669595f193"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'learning', 'Python', '.', 'It', 'is', 'one', 'of', 'the', 'most', 'popular', 'programming', 'languages']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(sentence_words, stop_words):\n",
        "    return ' '.join([word for word in sentence_words if \\\n",
        "                     word not in stop_words])"
      ],
      "metadata": {
        "id": "SbIaDxjgzrIA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(remove_stop_words(sentence_words,stop_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ix5hXfDfzv-9",
        "outputId": "88d999ed-f603-4cf1-c9be-f0be6b9235f7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I learning Python . It one popular programming languages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words.extend(['I','It', 'one'])\n",
        "print(remove_stop_words(sentence_words,stop_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lY43khz15YW",
        "outputId": "e607e953-9179-495b-c25a-91591fc199b0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning Python . popular programming languages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 1.05: Text Normalization**\n",
        "\n",
        "Normalize some given text. Make it more human sounding."
      ],
      "metadata": {
        "id": "pgA38OdtVvQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I visited the US from the UK on 22-10-18\""
      ],
      "metadata": {
        "id": "KKTIFeuu2ACb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(text):\n",
        "    return text.replace(\"US\", \"United States\")\\\n",
        "               .replace(\"UK\", \"United Kingdom\")\\\n",
        "               .replace(\"-18\", \"-2018\")"
      ],
      "metadata": {
        "id": "Qrub4Go52FJA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_sentence = normalize(sentence)\n",
        "print(normalized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0T6Inw32JC0",
        "outputId": "8805fdb5-5e94-4110-b3cb-7dbf0865a828"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I visited the United States from the United Kingdom on 22-10-2018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_sentence = normalize('US and UK are two superpowers')\n",
        "print(normalized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqc1hfBx2K-O",
        "outputId": "4c3b9202-2eb2-4c51-bd70-6ab9b9bcb127"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "United States and United Kingdom are two superpowers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 1.06: Spelling correction of a word and a Sentence**\n",
        "\n",
        "Spelling corrections on a word and a sentence using Python's autocorrect library."
      ],
      "metadata": {
        "id": "UsgrQ4ftJQGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries\n",
        "!pip install autocorrect\n",
        "!pip install nltk\n",
        "\n",
        "# Import the libraries\n",
        "from nltk.tokenize import word_tokenize\n",
        "from autocorrect import Speller\n",
        "\n",
        "# Download NLTK data if needed\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Example usage\n",
        "text = \"This is an exmaple sentence with some errrors.\"\n",
        "speller = Speller()\n",
        "corrected_text = speller(text)\n",
        "print(\"Original:\", text)\n",
        "print(\"Corrected:\", corrected_text)\n",
        "\n",
        "tokens = word_tokenize(corrected_text)\n",
        "print(\"Tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_NwSpLL2U80",
        "outputId": "45cb119d-1f8d-4001-c2e7-21a7351273ff"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622364 sha256=0c003bf612aa6f771608cccc4754dc5efa1ca8fa4d06332420b3486d558e9485\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/7b/6d/b76b29ce11ff8e2521c8c7dd0e5bfee4fb1789d76193124343\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.6.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Original: This is an exmaple sentence with some errrors.\n",
            "Corrected: This is an example sentence with some errors.\n",
            "Tokens: ['This', 'is', 'an', 'example', 'sentence', 'with', 'some', 'errors', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L4KTFgWI-UQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spell = Speller(lang='en')\n",
        "spell('Natureal')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sKLPEekq-iAS",
        "outputId": "56c187a1-00b9-43a4-9a14-0dca86c3cbdd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Natural'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = word_tokenize(\"Ntural Luanguage Processin deals with \"\\\n",
        "                         \"the art of extracting insightes from \"\\\n",
        "                         \"Natural Languaes\")"
      ],
      "metadata": {
        "id": "Xt34-7X7-lxb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EVSRA5i-pwi",
        "outputId": "27127c52-1e78-4660-e72e-5f2f21af5bcd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ntural', 'Luanguage', 'Processin', 'deals', 'with', 'the', 'art', 'of', 'extracting', 'insightes', 'from', 'Natural', 'Languaes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_spelling(tokens):\n",
        "    sentence_corrected = ' '.join([spell(word) \\\n",
        "                                   for word in tokens])\n",
        "    return sentence_corrected"
      ],
      "metadata": {
        "id": "0qzP7eWz-tSg"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(correct_spelling(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbraBKhX-uz6",
        "outputId": "a7b5d5a5-fd34-4e85-99df-4f3af866fbcb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural Language Processing deals with the art of extracting insights from Natural Languages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "p-7ro2nSIvQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 1.07: Using Stemming**\n",
        "\n",
        "Using a few fords in the stemming process so they get converted into their base forms"
      ],
      "metadata": {
        "id": "3FUdVVsUIYWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import stem"
      ],
      "metadata": {
        "id": "ACpY2KQE_e9B"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stems(word,stemmer):\n",
        "    return stemmer.stem(word)\n",
        "porterStem = stem.PorterStemmer()\n",
        "get_stems(\"production\",porterStem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fNNbMR0z_nGN",
        "outputId": "ed7a00dd-9ce9-42d0-d1b6-c3469eb1108e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'product'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_stems(\"coming\",porterStem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "K_Fqs-a-_z6Y",
        "outputId": "d558474f-c67e-46e3-eb2b-a3b96ab57fd3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'come'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_stems(\"firing\",porterStem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9gjZdSs8_1iD",
        "outputId": "f76e46f6-46d3-4883-8a44-8ce58e40f0aa"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fire'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_stems(\"battling\",porterStem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sj8JYmyzAM5T",
        "outputId": "7f73ad82-903a-42a7-cbc1-0dbae649fc4d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'battl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = stem.SnowballStemmer(\"english\")\n",
        "get_stems(\"battling\",stemmer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zmI5dxLJAVcw",
        "outputId": "11d192f4-c6cc-474c-fc76-2fab55449d31"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'battl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 1.08: Extracting the Base word using Lemmatization**\n",
        "\n",
        "how to use lemmatization process to produce the proper form of a given word"
      ],
      "metadata": {
        "id": "fV9mm6yxH75o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import download\n",
        "download('wordnet')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrbuI0m8Aa4f",
        "outputId": "9b4ac41c-ba77-4918-ced1-a56aa64ac247"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "aVSIOOu1Aget"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lemma(word):\n",
        "    return lemmatizer.lemmatize(word)\n",
        "get_lemma('products')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LLst5N1hAjV4",
        "outputId": "ce9c772a-4736-455c-afe3-18dfdf740639"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'product'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_lemma('production')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3TJ5-xisAoJM",
        "outputId": "de4c008b-68a4-4952-d9e8-9d1185a5d431"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'production'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_lemma('coming')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Tmp_2_HuAqNJ",
        "outputId": "b0feacf8-95d5-4525-9b95-ae0c01cdf040"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'coming'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 1.09: Treating Named Entities**\n",
        "# **bold text** bold text\n",
        "Find the named entities in a given sentence."
      ],
      "metadata": {
        "id": "dwpQqfI1GMw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import download\n",
        "from nltk import pos_tag\n",
        "from nltk import ne_chunk\n",
        "from nltk import word_tokenize\n",
        "download('maxent_ne_chunker')\n",
        "download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJL_9jTTA78H",
        "outputId": "d7e8e564-3e8f-45fa-ad71-7d3f39148172"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"We are reading a book published by Fenago \"\\\n",
        "           \"which is based out of Birmingham.\""
      ],
      "metadata": {
        "id": "047f3nqvBnnH"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ner(text):\n",
        "    i = ne_chunk(pos_tag(word_tokenize(text)), binary=True)\n",
        "    return [a for a in i if len(a)==1]\n",
        "get_ner(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFGc7kDKBywe",
        "outputId": "4ff8534a-a6a9-4931-f03b-f15b94457cb8"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Tree('NE', [('Fenago', 'NNP')]), Tree('NE', [('Birmingham', 'NNP')])]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 1.10: Word Sense Disambiguation**\n",
        "\n",
        "use of the word \"bank\" in two different sentences."
      ],
      "metadata": {
        "id": "wwB4HdkYF0F-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.wsd import lesk\n",
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFxIVig8B-w9",
        "outputId": "aa3b851b-38a1-49dd-92e5-7303f4e3c1a8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = \"Keep your savings in the bank\"\n",
        "sentence2 = \"It's so risky to drive over the banks of the road\""
      ],
      "metadata": {
        "id": "a0DeIUvPCJnU"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synset(sentence, word):\n",
        "    return lesk(word_tokenize(sentence), word)\n",
        "get_synset(sentence1,'bank')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUsYdXPDCQwR",
        "outputId": "c6769a69-3176-4620-86c0-fd75dabba984"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('savings_bank.n.02')"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_synset(sentence2,'bank')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZR5qs9FECUrv",
        "outputId": "85247ef0-3953-4173-bc90-3d3895d3f4d2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('bank.v.07')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 1.11 Sentence Boundary Detection**\n",
        " Extract sentences from a paragraph using the sent_tokenize() method, used to detect sentence boundaries:"
      ],
      "metadata": {
        "id": "XhiHVsqvFTG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "e0rfUSvwCclM"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentences(text):\n",
        "    return sent_tokenize(text)\n",
        "get_sentences(\"We are reading a book. Do you know who is \"\\\n",
        "              \"the publisher? It is Fenago. Fenago is based \"\\\n",
        "              \"out of Birmingham.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lNxkNqfCtB8",
        "outputId": "5dd34301-c7dc-4e1c-d19c-44c425c6fada"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We are reading a book.',\n",
              " 'Do you know who is the publisher?',\n",
              " 'It is Fenago.',\n",
              " 'Fenago is based out of Birmingham.']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_sentences(\"Mr. Donald John Trump is the current \"\\\n",
        "              \"president of the USA. Before joining \"\\\n",
        "              \"politics, he was a businessman.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhKJbNqtCuiR",
        "outputId": "f72df1ac-049f-4897-d5fb-d16bd6fc0858"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mr. Donald John Trump is the current president of the USA.',\n",
              " 'Before joining politics, he was a businessman.']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T4liougNCxbM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}